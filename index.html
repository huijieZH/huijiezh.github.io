<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Huijie Zhang</title>
  
  <meta name="author" content="Huijie Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="keywords" content="huijie, huijie zhang, zhang huijie" />

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Huijie Zhang</name>
              </p>
              <p>I am an Ph.D. student at <a href="https://ece.engin.umich.edu/">University of Michigan, Ann Arbor</a>, supervised by <a href="https://qingqu.engin.umich.edu/">Prof. Qing Qu</a>. </p>
              <p>
                Previously, I obtained my Bachelor's degree in Mechanical Engineering 
                from <a href="http://english.hust.edu.cn/">Huazhong university of Science and Technology</a>, 
                advised by <a href="http://faculty.hust.edu.cn/wuzhigang/zh_CN/index.htm">Prof. Zhigang Wu</a>; Master's degree in Mechanical Engineering and Electrical and Computer Engineering from <a href="https://ece.engin.umich.edu/">University of Michigan, Ann Arbor</a>, advised by <a href="https://web.eecs.umich.edu/~ocj/">Prof. Chad Jenkins</a>
              </p>
              <p>
                My research interests lie in generative model and diffusion model. Recently, my project is related the <strong>empirical and theoretical analysis of low-dimensional structures</strong> in diffusion model, and its applications related to <strong>training efficiency</strong>, <strong>controllable generation</strong> and <strong>privacy</strong>. My previous works are related to 3D vision, robotics manipulation and reinforcement learning.
              </p>
              <p>
                [Updated in 09/2024]
              </p>
              <p style="text-align:center">
                <!-- <a href="data/huijiezh-CV.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=W9Mplc4AAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/huijieZH/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/huijiezh_circle2.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/huijiezh_circle2.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p style="text-align:left;font-size:small;">
              [09/2024] Our work on <a href="https://arxiv.org/abs/2409.02374">LOCO Edit</a> was accepted by NeurIPS 2024</a>!
            </p>
            <p style="text-align:left;font-size:small;">
              [05/2024] Our work on <a href="https://arxiv.org/abs/2310.05264">Diffusion Model Reproducibility</a> was accepted by ICML 2024</a>!
            </p>
            <p style="text-align:left;font-size:small;">
              [02/2024] Our work on <a href="https://arxiv.org/abs/2312.09181">Multi-stage Diffusion Model</a> was accepted by CVPR 2024!
            </p>
            <p style="text-align:left;font-size:small;">
              [10/2023] Our work on <a href="https://arxiv.org/abs/2310.05264">Diffusion Model Reproducibility</a> was accepted by <a href="https://diffusionworkshop.github.io/">NeurIPS2023 Workshop</a>, received <strong> best paper award </strong>!
            </p>
            <p style="text-align:left;font-size:small;">
              [08/2022] Our work on <a href="https://arxiv.org/abs/2208.10002">TransNet</a> was accepted by ECCV 2022 Workshop!
            </p>
            <p style="text-align:left;font-size:small;">
              [07/2022] Our work on <a href="https://arxiv.org/abs/2203.03890">Clearpose</a>  was accepted by ECCV 2022!
            </p>
            <p style="text-align:left;font-size:small;">
              [06/2022] Our work on <a href="https://arxiv.org/abs/2203.00283">Progresslabeller</a> was accepted by IROS 2022!
            </p>

          </td>

        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publication</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <!-- Subspace Clustering -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div >
              <img src='images/subspace.jpg' width="250">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2409.02426">
              <papertitle>Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering</papertitle>
            </a>
            <br>
            <a href="https://peng8wang.github.io/">Peng Wang*</a>,
            <strong>Huijie Zhang*</strong>, 
            <a href="https://scholar.google.com/citations?user=-D1mYhIAAAAJ&hl=en">Zekai Zhang</a>,
            <a href="https://chicychen.github.io/">Siyi Chen</a>,
            <a href="https://people.eecs.berkeley.edu/~yima/">Yi Ma</a>,
            <a href="https://qingqu.engin.umich.edu/">Qing Qu</a>
            <br>
            In submission to <em>ICLR</em>, 2025
            <br>
            <a href="https://arxiv.org/abs/2409.02426">ArXiv</a>
            /<a href="https://github.com/huijieZH/Diffusion-Model-Generalizability">Code</a>
            <p></p>
            <p>
              In this work, we provide theoretical insights into the connection between diffusion model and subspace clustering. The connection shed light into the transition of diffusion model from memorization to generalization and the mechanism it breaks the curse of dimensionality.
            </p>
          </td>
        </tr>

        <!-- LOCO-Edit -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div >
              <img src='images/loco_edit.png' width="250">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2409.02374">
              <papertitle>Exploring Low-Dimensional Subspaces in Diffusion Models for Controllable Image Editing</papertitle>
            </a>
            <br>
            <a href="https://chicychen.github.io/">Siyi Chen*</a>
            <strong>Huijie Zhang*</strong>, 
            <a href="https://www.linkedin.com/in/minzhe-guo/">Minzhe Guo</a>,
            <a href="https://scholar.google.com/citations?user=ybsmKpsAAAAJ&hl=en">Yifu Lu</a>,
            <a href="https://peng8wang.github.io/">Peng Wang</a>, 
            <a href="https://qingqu.engin.umich.edu/">Qing Qu</a>
            <br>
            <em>NeurIPS</em>, 2024
            <br>
            <a href="https://arxiv.org/abs/2409.02374">ArXiv</a>
            /<a href="https://github.com/ChicyChen/LOCO-Edit">Code</a>
            <p></p>
            <p>
              We improve the understanding of the semantic space in diffusion model and proposed <strong>LOCO Edit</strong>, an editing method achieving precise and disentangled image editing without additional training. The proposed method is also supported by theoretical justification and has nice properties: homogeneity, transferability, composability, and linearity.
            </p>
          </td>
        </tr>

          <!-- Reproducibility -->
          <tr onmouseout="reproducibility_stop()" onmouseover="reproducibility_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div >
                <img src='images/reproducibility.png' width="250">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2310.05264">
                <papertitle>The Emergence of Reproducibility and Consistency in Diffusion Models</papertitle>
              </a>
              <br>
              <strong>Huijie Zhang*</strong>, 
              <a href="https://scholar.google.com/citations?user=O3Df4PwAAAAJ&hl=zh-CN">Jinfan Zhou*</a>,
              <a href="https://scholar.google.com/citations?user=ybsmKpsAAAAJ&hl=en">Yifu Lu</a>, 
              <a href="https://www.linkedin.com/in/minzhe-guo/">Minzhe Guo</a>, <br>
              <a href="https://peng8wang.github.io/">Peng Wang</a>, 
              <a href="https://liyueshen.engin.umich.edu/">Liyue Shen</a>,
              <a href="https://qingqu.engin.umich.edu/">Qing Qu</a>
              <br>
              <em>NeurIPS Workshop</em>, 2023 (<strong>best paper award</strong>); <em>ICML</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2310.05264">ArXiv</a>
              /<a href="https://ece.engin.umich.edu/stories/genai-diffusion-models-learn-to-generate-new-content-more-consistently-than-expected">News</a>
              /<a href="https://event.baai.ac.cn/activities/749">Talk</a>
              /<a href="https://github.com/huijieZH/Diffusion-Model-Generalizability">Code</a>
              <p></p>
              <p>
                We investigate an intriguing and prevalent phenomenon of diffusion models: given the same starting noise input and a deterministic sampler, different diffusion models often yield remarkably similar outputs.
              </p>
            </td>
          </tr>

          <!-- Multistage -->
          <tr onmouseout="multistage_stop()" onmouseover="multistage_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div >
                <img src='images/multistage.png' width="250">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.09181">
                <papertitle>Improving Training Efficiency of Diffusion Models via Multi-Stage Framework
                  and Tailored Multi-Decoder Architecture</papertitle>
              </a>
              <br>
              <strong>Huijie Zhang*</strong>, 
              <a href="https://scholar.google.com/citations?user=ybsmKpsAAAAJ&hl=en">Yifu Lu*</a>, 
              <a href="https://sites.google.com/view/ismailalkhouri/about">Ismail Alkhouri</a>, 
              <a href="https://sites.google.com/site/sairavishankar3/">Saiprasad Ravishankar</a>, <br>
              <a href="https://sites.google.com/view/dogyoonsong/home">Dogyoon Song</a>,
              <a href="https://qingqu.engin.umich.edu/">Qing Qu</a>
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2312.09181">ArXiv</a>
              /<a href="https://www.huijiezh.com/multistage-diffusion-model/">Website</a>
              /<a href="https://github.com/LucasYFL/Multistage_Diffusion">Github</a>
              /<a href="https://zhidx.com/p/426852.html">Talk</a>
              <p></p>
              <p>
                In this study, we significantly enhance the training and sampling efficiency of diffusion models
                through a novel multi-stage framework. This method divides the time interval into several stages,
                using a specialized multi-decoder U-net architecture that combines time-specific models with a
                common encoder for all stages.
              </p>
            </td>
          </tr>



          <!-- TransNet -->
          <tr onmouseout="transnet_stop()" onmouseover="transnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div >
                <img src='images/transnet.png' width="250">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2208.10002">
                <papertitle>TransNet: Category-Level Transparent Object Pose Estimation</papertitle>
              </a>
              <br>
              <strong>Huijie Zhang</strong>, 
              <a href="https://topipari.com/">Anthony Opipari</a>,
              <a href="https://sites.google.com/umich.edu/xiaotong-chen/home">Xiaotong Chen</a>, <br>
              <a href="https://jiyuezh.github.io/">Jiyue Zhu</a>,
              <a href="https://zerenyu.github.io/">Zeren Yu</a>, 
              <a href="https://ocj.name/">Odest Chadwicke Jenkins</a>,
              <br>
              <em>ECCV Workshop</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2208.10002">ArXiv</a>
              /<a href="https://progress.eecs.umich.edu/projects/transnet/">Website</a>
              <p></p>
              <p>
                We proposed TransNet, a two-stage pipeline that learns to estimate category-level transparent object pose using localized depth completion and surface normal estimation. 
              </p>
            </td>
          </tr>

          <!-- ClearPose -->
          <tr onmouseout="clearpose_stop()" onmouseover="clearpose_start()">
            <td style="padding:20px;width:25%;vertical-align:middle;">
              <div class="one">
                <div class="two" id='clearpose_image'>
                  <img src='images/clearpose_after.png' width="250"></div>
                <img src='images/clearpose_before.png' width="250">
              </div>
              <script type="text/javascript">
                function clearpose_start() {
                  document.getElementById('clearpose_image').style.opacity = "1";
                }

                function clearpose_stop() {
                  document.getElementById('clearpose_image').style.opacity = "0";
                }
                clearpose_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;">
              <a href="https://arxiv.org/abs/2203.03890">
                <papertitle>ClearPose: Large-scale Transparent Object Dataset and Benchmark</papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/umich.edu/xiaotong-chen/home">Xiaotong Chen</a>,
              <strong>Huijie Zhang</strong>, 
							<a href="https://zerenyu.github.io/">Zeren Yu</a>, <br>
              <a href="https://topipari.com/">Anthony Opipari</a>,
              <a href="https://ocj.name/">Odest Chadwicke Jenkins</a>,
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2203.03890">ArXiv</a>
              /<a href="https://github.com/opipari/ClearPose">Github</a>
              /<a href="https://progress.eecs.umich.edu/projects/clearpose/#cite">Website</a>
              <p></p>
              <p>
              We collected a large-scale transparent object dataset with RGB-D and annotated poses. 
              And we benchmarked transparent object depth completion and poes estimation on this dataset.
              </p>            
            </td>
          </tr>
					
          <!-- ProgressLabeller -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div >
                <img src='images/progresslabeller.png' width="250">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2203.00283">
                <papertitle>ProgressLabeller: Visual Data Stream Annotation for Training Object-Centric 3D Perception</papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/umich.edu/xiaotong-chen/home">Xiaotong Chen</a>,
              <strong>Huijie Zhang</strong>, 
							<a href="https://zerenyu.github.io/">Zeren Yu</a>, <br>
              <a href="https://scholar.google.com/citations?user=D8nfug0AAAAJ&hl=en">Stanley Lewis</a>,
              <a href="https://ocj.name/">Odest Chadwicke Jenkins</a>,
              <br>
              <em>IROS</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2203.00283">ArXiv</a>
              /<a href="https://github.com/huijieZH/ProgressLabeller">Github</a>
              /<a href="https://progress.eecs.umich.edu/projects/progress-labeller/">Website</a>
              <p></p>
              <p>
              ProgressLabeller is an efficient 6D pose annotation method. It is also the first open source tools compatible with transparent object. 
              It was implemented as a blender Add-on, more user-friendly for using.
              </p>
            </td>
          </tr>

          <!-- <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/nerf_supervision.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nerf_supervision.jpg' width="320">
              </div>
              <script type="text/javascript">
                function nerfsuper_start() {
                  document.getElementById('nerfsuper_image').style.opacity = "1";
                }

                function nerfsuper_stop() {
                  document.getElementById('nerfsuper_image').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://waymo.com/research/block-nerf/">
                <papertitle>NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields</papertitle>
              </a>
              <br>
              <a href="https://yenchenlin.me/">Lin Yen-Chen</a>, 
              <a href="http://www.peteflorence.com/">Pete Florence</a>, 
              <strong>Jonathan T. Barron</strong>,  <br>
              <a href="https://scholar.google.com/citations?user=_BPdgV0AAAAJ&hl=en">Tsung-Yi Lin</a>, 
              <a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>,
              <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>
              <br>
              <em>ICRA</em>, 2022  
              <br>
							<a href="http://yenchenlin.me/nerf-supervision/">project page</a> / 
							<a href="https://arxiv.org/abs/2203.01913">arXiv</a> / 
							<a href="https://www.youtube.com/watch?v=_zN-wVwPH1s">video</a> /
							<a href="https://github.com/yenchenlin/nerf-supervision-public">code</a> / 
							<a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">colab</a>				
              <p></p>
              <p>NeRF works better than RGB-D cameras or multi-view stereo when learning object descriptors.</p>
            </td>
          </tr> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Project</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>			
			
          <tr onmouseout="minecraft_stop()" onmouseover="minecraft_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='minecraft_image'>
                  <img src='images/minecraft.gif' width="250"></div>
                <img src='images/minecraft.png' width=250">
              </div>
              <script type="text/javascript">
                function minecraft_start() {
                  document.getElementById('minecraft_image').style.opacity = "1";
                }

                function minecraft_stop() {
                  document.getElementById('minecraft_image').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            
            <td style="padding:20px;width:60%;vertical-align:middle">
							<a href="https://github.com/huijieZH/Deep-reinforcement-learning-on-Minecraft">
                <papertitle>Deep Q-learning from demonstration on
                  Minecraft</papertitle>
              </a>
              <br>
              <strong>Huijie Zhang</strong>, <a href="https://scholar.google.com/citations?user=3nL-yukAAAAJ&hl=en">Phil Kangle Mu</a>, Ying Jiang, Sihang Wei,
              <br>
              <a href="https://github.com/huijieZH/Deep-reinforcement-learning-on-Minecraft">Github</a>
              <p></p>

              <p>This Project useed Deep Q-learning from demonstration to teach agent cutting trees in Minecraft Environment</p>
            </td>
          </tr>


        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <br>
                This website has been inspired by <a href="https://jonbarron.info/">Jon Barron</a>. 
                <!-- <br>
                Highly appreciate <a href="https://zerenyu.github.io/">Zeren Yu</a> for the homepage photo shoot. -->
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
